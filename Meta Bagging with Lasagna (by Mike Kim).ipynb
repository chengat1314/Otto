{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import nesterov_momentum, adagrad\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from scipy.special import expit\n",
    "import random\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.random_projection import SparseRandomProjection, GaussianRandomProjection\n",
    "from sklearn.manifold import LocallyLinearEmbedding, MDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.kernel_approximation import RBFSampler, Nystroem\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "\n",
    "random.seed(21)\n",
    "np.random.seed(21)\n",
    "\n",
    "LINES = 61877\n",
    "\n",
    "def load_train_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    np.random.shuffle(X)\n",
    "    X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(labels).astype(np.int32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(np.log(1+X))\n",
    "    rbm1 = SVC(C=100.0, gamma = 0.1, probability=True, verbose=1).fit(X[0:9999,:], y[0:9999])\n",
    "    rbm2 = RandomForestClassifier(n_estimators=300, criterion='entropy', max_features='auto', bootstrap=False, oob_score=False, n_jobs=1, verbose=1).fit(X[0:9999,:], y[0:9999])\n",
    "    rbm3 = GradientBoostingClassifier(n_estimators=50,max_depth=11,subsample=0.8,min_samples_leaf=5,verbose=1).fit(X[0:9999,:], y[0:9999])\n",
    "    X =  np.append(X[10000:LINES,:], np.power(rbm1.predict_proba(X[10000:LINES,:])*rbm2.predict_proba(X[10000:LINES,:])*rbm3.predict_proba(X[10000:LINES,:]), (1/3.0))   , 1)\n",
    "    return X, y[10000:LINES], encoder, scaler, rbm1, rbm2, rbm3\n",
    "\n",
    "def load_test_data(path, scaler, rbm1, rbm2, rbm3):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n",
    "    X = scaler.transform(np.log(1+X))\n",
    "    X =  np.append(X, np.power(rbm1.predict_proba(X)*rbm2.predict_proba(X)*rbm3.predict_proba(X), (1/3.0)), 1)\n",
    "    return X, ids\n",
    "\n",
    "def make_submission(y_prob, ids, encoder, name='/home/mikeskim/Desktop/kaggle/otto/data/lasagneSeed21.csv'):\n",
    "    with open(name, 'w') as f:\n",
    "        f.write('id,')\n",
    "        f.write(','.join(encoder.classes_))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([id] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote submission to file {}.\".format(name))\n",
    "\n",
    "\n",
    "#Load Data\n",
    "X, y, encoder, scaler, rbm1, rbm2, rbm3 = load_train_data('/home/mikeskim/Desktop/kaggle/otto/data/train.csv')\n",
    "X_test, ids = load_test_data('/home/mikeskim/Desktop/kaggle/otto/data/test.csv', scaler, rbm1, rbm2, rbm3)\n",
    "\n",
    "num_classes = len(encoder.classes_)\n",
    "num_features = X.shape[1]\n",
    "\n",
    "print(num_classes); print(num_features); print(X)\n",
    "\n",
    "\n",
    "layers0 = [('input', InputLayer),\n",
    "('dropoutf', DropoutLayer),\n",
    "('dense0', DenseLayer),\n",
    "('dropout', DropoutLayer),\n",
    "('dense1', DenseLayer),\n",
    "('dropout2', DropoutLayer),\n",
    "('dense2', DenseLayer),\n",
    "('output', DenseLayer)]\n",
    "\n",
    "\n",
    "net0 = NeuralNet(layers=layers0,\n",
    "\n",
    "input_shape=(None, num_features),\n",
    "dropoutf_p=0.1,\n",
    "dense0_num_units=600,\n",
    "dropout_p=0.3,\n",
    "dense1_num_units=600,\n",
    "dropout2_p=0.1,\n",
    "dense2_num_units=600,\n",
    "\n",
    "output_num_units=num_classes,\n",
    "output_nonlinearity=softmax,\n",
    "\n",
    "#update=nesterov_momentum,\n",
    "update=adagrad,\n",
    "update_learning_rate=0.008,\n",
    "eval_size=0.2,\n",
    "verbose=1,\n",
    "max_epochs=20)\n",
    "\n",
    "\n",
    "\n",
    "net0.fit(X, y)\n",
    "y_prob = net0.predict_proba(X_test)\n",
    "num_runs = 50\n",
    "\n",
    "for jj in xrange(num_runs):\n",
    "  print(jj)\n",
    "  X, y, encoder, scaler, rbm1, rbm2, rbm3 = load_train_data('/home/mikeskim/Desktop/kaggle/otto/data/train.csv')\n",
    "  X_test, ids = load_test_data('/home/mikeskim/Desktop/kaggle/otto/data/test.csv', scaler, rbm1, rbm2, rbm3)\n",
    "  num_classes = len(encoder.classes_)\n",
    "  num_features = X.shape[1]\n",
    "  net0.fit(X, y)\n",
    "  y_prob = y_prob + net0.predict_proba(X_test)\n",
    "\n",
    "\n",
    "y_prob = y_prob/(num_runs+1.0)\n",
    "make_submission(y_prob, ids, encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
